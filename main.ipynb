{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Between Semantic Similarity and Wikipedia Distance\n",
    "### CMSC 320, Spring 2022\n",
    "#### Ben Landrum, Gaetan Almela, and Nav Bindra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikipedia link graph is a graph where vertices represent Wikipedia articles and directed edges from vertex $A$ to vertex $B$ represent a link to $B$ in the text of $A$. We define Wikipedia distance between pages $A$ and $B$ to be the minimum number of links required to get from $A$ to $B$ in the Wikipedia link graph, where the first such link is on page $A$, the second is on the page the first link points to, and so on, until a link to $B$ is found.\n",
    "\n",
    "Word2Vec is a popular tool for computing word embeddings, which use a vector space representation of words to place them in an abstract space where similar vectors indicate that words appear in similar contexts. These vectors can then be compared to find a metric of semantic similarity between words. \n",
    "\n",
    "Because pages on Wikipedia contain links which are relevant to the topic the page is covering, similar topics are often closely if not directly linked. Therefore, it stands to reason that if semantic similarity is a valid metric of similarity between topics, a higher semantic similarity between two topics should correspond to a lower Wikipedia distance between said topics. We are looking to test the hypothesis that there exists a negative correlation between Wikipedia distance and Word2Vec semantic similarity. Our null hypothesis is that there does not exist a statistically significant negative correlation between Wikipedia distance and Word2Vec semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Semantic Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This is not hard. If we wanted to pad it we could compute our own embedding, which would probably make it seem a little less trivial and would honestly be really easy, just involves a lot of text being loaded.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Wikipedia Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top $n$ Wikipedia Articles\n",
    "\n",
    "English Wikipedia has over 6 million articles. A graph describing the links between all of them would be prohibitively large, so we will only consider the top $n$ articles for some large $n$. For $n > 1,000$ this is not something that can be found online, so we must compute it ourselves.\n",
    "\n",
    "Wikipedia provides data dumps describing page views in the form of files tallying how many views each page received in a given hour. These files are available at [https://dumps.wikimedia.org/other/pageviews/](https://dumps.wikimedia.org/other/pageviews/), and date back to May 1st, 2015. The most popular pages on Wikipedia at any given time are generally determined by current events, so to factor out this bias and get a representative sample of the most popular articles we want to sample the largest possible time period, in this case May 1st, 2015 to the present day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from faker import Faker\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_WIKIS = ['en', 'en.m'] # desktop and mobile versions of english wikipedia\n",
    "PAGE_TYPE_REGEX = '^((User)|(Talk)|(Wikipedia)|(Special)|(Portal)|(File)):' # regex for filtering non-article pages\n",
    "N = 10 ** 4 # number of pages to return\n",
    "OUTPUT_FILE = 'wiki_top_k.csv' # name of output file\n",
    "DATA_DIR = 'Data' # directory where the data is stored\n",
    "RANDOM_FILES = 6 # Number of random files to use\n",
    "\n",
    "# creating a Faker object which will later be used to generate random datetimes\n",
    "fake = Faker()\n",
    "\n",
    "# suppressing a warning caused by use of match groups in a regex \n",
    "warnings.filterwarnings('ignore', message=\"This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random datetime for which there exists a pageview file\n",
    "date = fake.date_time_between(start_date=datetime.datetime(year=2015, month=5, day=1, hour=1), end_date='now')\n",
    "\n",
    "# get the url of the corresponding pageview file\n",
    "url = f\"https://dumps.wikimedia.org/other/pageviews/{date.year}/{date.year}-{date.month:02d}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
    "\n",
    "# download and save the file\n",
    "seed_file = f\"{DATA_DIR}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
    "r = get(url)\n",
    "with open(seed_file, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# importing first file to accumulate the others from\n",
    "all_views = pd.read_csv(seed_file, sep=\" \", header=None, names=[\"project\", \"page\", \"views\", \"null\"], usecols=[\"project\", \"page\", \"views\"])\n",
    "\n",
    "# filtering to only pages from english wikipedia, both desktop and mobile\n",
    "en_views = all_views[all_views['project'].isin(ENGLISH_WIKIS)]\n",
    "# filtering user, talk and other non-article pages\n",
    "views = en_views[~(en_views['page'].str.contains(PAGE_TYPE_REGEX, na=False))]\n",
    "\n",
    "# computing views for each page \n",
    "views = views.groupby('page').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the random files and add their views to the seed file, which becomes increasingly expensive as `all_views` grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017/04/07:13\n",
      "[1]\t2017/04/07:13\t2,448,411 pages\t[0:00:22.516213 elapsed, 0.00% complete, 0:00:22.516213 remaining]\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now() # for timing execution\n",
    "for i in range(RANDOM_FILES - 1):\n",
    "    # generate a random datetime for which there exists a pageview file\n",
    "    date = fake.date_time_between(start_date=datetime.datetime(year=2015, month=5, day=1, hour=1), end_date='now')\n",
    "\n",
    "    # get the url of the corresponding pageview file\n",
    "    url = f\"https://dumps.wikimedia.org/other/pageviews/{date.year}/{date.year}-{date.month:02d}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
    "\n",
    "    # download and save the file\n",
    "    r = get(url)\n",
    "    with open(f\"{DATA_DIR }/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\", 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"{date.strftime('%Y/%m/%d:%H')}\")\n",
    "    \n",
    "    # read the file (pandas can handle gzip directly) and perform same operations as above\n",
    "    try:\n",
    "        df = pd.read_csv(f\"{DATA_DIR}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\", compression='gzip', sep=\" \", header=None, names=[\"project\", \"page\", \"views\", \"null\"], usecols=[\"project\", \"page\", \"views\"])\n",
    "    except: # handling error caused by pandas' \"C\" CSV engine\n",
    "        print(f\"{date.strftime('%Y/%m/%d:%H')} failed\" )\n",
    "        os.remove(f\"{DATA_DIR}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\")\n",
    "        continue\n",
    "\n",
    "    df = df[df['project'].isin(ENGLISH_WIKIS)]\n",
    "    df = df[~(df['page'].str.contains(PAGE_TYPE_REGEX, na=False))]\n",
    "    df = df.groupby('page').sum()\n",
    "    # merging with previous dataframe\n",
    "    views = views.add(df, fill_value=0)\n",
    "\n",
    "    # print status report\n",
    "    print(f\"[{i + 1}]\\t{date.strftime('%Y/%m/%d:%H')}\\t{len(views):,} pages\\t[{datetime.datetime.now() - start_time} elapsed, {(i * 100) / RANDOM_FILES:.2f}% complete, {(datetime.timedelta(seconds=((datetime.datetime.now() - start_time).total_seconds() / (i + 1)) * (RANDOM_FILES - i - 1)))} remaining]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the pages and their views to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views.sort_values('views', ascending=False, inplace=True)\n",
    "\n",
    "views.head(N).to_csv(OUTPUT_FILE, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the form shown here, this code only samples data from 6 hours, but for a more complete sample a more optimized version of this code was run over 12 hours, sampling a total of 1,105 hours of data. The top 5 million articles from this dataset are in the file `raw_views_5M.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start scraping Wikipedia, we have to define some constants for our scraper. Scraping Wikipedia is suprisingly easy, the DOM tree and class names are very consistent. Despite this, Wikipedia still has a lot of garbage links that we don't want. This is where we define all of our rules for what counts as a \"bad\" link.  Examples of bad links include images, self links, internal links (links to enlarge photos, believe it or not), or invisible links that take you to the top of the page.\n",
    "\n",
    "One might think to simply filter by links containing `/wiki/`, but even those links aren't safe. Indeed Wikipedia contains various different types of internal `/wiki/` links that point to various medias, template pages, or help files, which we are not interested in. We also filter them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "GRAPH_FILENAME = 'wiki_graph'\n",
    "\n",
    "# number of pages to experiment on\n",
    "N = 1000\n",
    "\n",
    "# we filter out these types of link selectors\n",
    "CLEAN = [\n",
    "  'a[id=\"top\"]',\n",
    "  'a[class=\"mw-selflink selflink\"]',\n",
    "  'a[class=\"image\"]',\n",
    "  'a[class=\"internal\"]',\n",
    "]\n",
    "\n",
    "# we filter out these links\n",
    "REM_LINKS = [\n",
    "  r\"(\\/wiki\\/File:\\w+)\",\n",
    "  r\"(\\/wiki\\/Special:\\w+)\",\n",
    "  r\"(\\/wiki\\/Template:\\w+)\",\n",
    "  r\"(\\/wiki\\/Category:\\w+)\",\n",
    "  r\"(\\/wiki\\/Portal:\\w+)\",\n",
    "  r\"(\\/wiki\\/Template_talk:\\w+)\",\n",
    "  r\"(\\/wiki\\/Help:\\w+)\",\n",
    "  r\"(\\/wiki\\/Wikipedia:\\w+)\",\n",
    "  r\"(^#\\w+)\",\n",
    "]\n",
    "\n",
    "# main page content selector\n",
    "CONT_SEL = \"div#content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load the list of pages that we are interested in scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load most popular wikipedia pages csv\n",
    "df = pd.read_csv(f\"{DATA_DIR}/{OUTPUT_FILE}.csv\").head(N)\n",
    "\n",
    "# get a list of pages as an array of strings\n",
    "urls = df['article'].to_numpy().astype(str)\n",
    "\n",
    "# filter the pages to only articles without ':' in the title\n",
    "# I know this may not cover everything, but I'm just testing here\n",
    "urls = urls[np.char.find(urls, ':') == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Pages Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overwhelming Wikipedia we download every page, unmodified, locally. This allows us to fail multiples times without Wikipedia having to know about it. This is also much faster since we are working with local files. The reason we leave the files unmodified here instead of cleaning them at the same time is that we always want to have the maximum amount of information possible first (in case you need it) and clean it up as a separate step.\n",
    "\n",
    "One thing to note here is that we also add a variable `START_INDEX` to manually start from a particular index in our list of links to scrape. This serves two purposes. The first is in case a special page break our code, we don't have to restart from the beginning to rescrape the pages we already have. The second is for this page not to take 50 hours to compile to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you wanna skip ahead\n",
    "START_INDEX = 970\n",
    "SLEEP_TIME_S = 2 # dont make less than 2, we dont wanna overwhelm wikipedia too much\n",
    "\n",
    "n = urls.size\n",
    "\n",
    "# compile the title cleanup regex for optimization\n",
    "title_re = re.compile(r\"\\/\")\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "  if i < START_INDEX:\n",
    "    continue\n",
    "\n",
    "  print(f\"{i} of {n} ({round((i / n) * 100, 2)}%) - Scraping {url}\")\n",
    "\n",
    "  # load the page as html with BeautifulSoup\n",
    "  res = get(f'https://en.wikipedia.org/wiki/{url}')\n",
    "\n",
    "  # check if we got baned :c\n",
    "  if res.status_code != 200:\n",
    "    print(\"We got got\")\n",
    "    break\n",
    "\n",
    "  html = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "  # save html page as string\n",
    "  html_str = str(html.prettify())\n",
    "\n",
    "  # replace bad characters in titles with underscores\n",
    "  title = title_re.sub(\"_\", url)\n",
    "\n",
    "  # save file\n",
    "  f = open(f\"{DATA_DIR}/pages/{title}.html\", \"w\")\n",
    "  f.write(html_str)\n",
    "  f.close()\n",
    "\n",
    "  # let's not overload wikipedia with requests here\n",
    "  time.sleep(SLEEP_TIME_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use our local data and our filters defined above to build our page connections graph with `networkx`. We itterate through each page in our local file structure and, for each page we look at each link that matches our filters defined in our constants above. Note that we don't add every url that passes our filter here, _we only add an edge if its destination page is also in our list of pages_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.scandir(path = f\"./{DATA_DIR}/pages/\")\n",
    "n = len(list(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "# list our downloaded files\n",
    "files = os.scandir(path = f\"./{DATA_DIR}/pages/\")\n",
    "# n = len(list(files))\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "  # extract url from filename\n",
    "  url = Path(file).stem\n",
    "\n",
    "  print(f\"{i} of {n} ({round((i / n) * 100, 2)}%) - Graphing {url}\")\n",
    "\n",
    "  # add the current page to the graph\n",
    "  G.add_node(url)\n",
    "\n",
    "  # load the page into memory\n",
    "  f = open(file, \"r\")\n",
    "  html = BeautifulSoup(f.read(), 'html.parser')\n",
    "  f.close()\n",
    "\n",
    "  # clean up unwanted elements from page\n",
    "  for c in CLEAN:\n",
    "    els = html.select(f\"{CONT_SEL} {c}\")\n",
    "    for el in els:\n",
    "      el.decompose()\n",
    "\n",
    "  # further remove unwanted links\n",
    "  links = html.select(f\"{CONT_SEL} a\")\n",
    "  for link in links:\n",
    "    # extract href from link\n",
    "    href = link['href']\n",
    "\n",
    "    # we are only interested in links that start with `/wiki/` and aren't in REM_LINKS\n",
    "    if not any([re.match(regex, href) for regex in REM_LINKS]) and re.match(r\"^\\/wiki\\/\\w+\", href):\n",
    "      # extract href from link\n",
    "      href = link['href']\n",
    "\n",
    "      # this is the destination url\n",
    "      url_ = re.sub(r\"\\/wiki\\/\", \"\", href)\n",
    "\n",
    "      # we only add the destination url if it happens to be inside our links of interests\n",
    "      if url_ in urls:\n",
    "        # add edge to graph\n",
    "        G.add_edge(url, url_)\n",
    "\n",
    "  # save our graph at other file\n",
    "  if i % 3 == 0:\n",
    "    nx.write_gml(G, f\"{DATA_DIR}/{GRAPH_FILENAME}.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we draw a graph of the top 1000 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = np.array(G.nodes)\n",
    "\n",
    "sub_nodes = np.random.choice(nodes, size = 25)\n",
    "sub_nodes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "nodes = (G.nodes)\n",
    "N_nodes = G.number_of_nodes()\n",
    "\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "# Setting up the text by using node position \n",
    "texts=[\n",
    "  plt.text(\n",
    "    pos[node][0], \n",
    "    pos[node][1],\n",
    "    str(node),\n",
    "    rotation=(i/N_nodes)*360,\n",
    "    fontsize=10,horizontalalignment='center',\n",
    "    verticalalignment='center'\n",
    "  )\n",
    "  \n",
    "  for i, node in enumerate(nodes)\n",
    "]\n",
    "\n",
    "nx.draw(G, pos=pos, arrows=True, node_size=50, linewidths=0.6, ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test\n",
    "\n",
    "We now proceed to test our hypothesis that the distance between a pair of articles on the Wikipedia graph is correlated with the semantic similarity between the titles of the articles. \n",
    "\n",
    "A slight issue is presented by the fact that Wikipedia distance is directional, whereas the semantic similarity is not. As a result, it's possible that the distance from article $A$ to article $B$ may differ from the distance from article $B$ to article $A$. Because the direction of a pair of articles is sampled randomly, there is no bias introduced by this unidirectional sample, so it's acceptable to compare only one direction's distance to the semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate k article pairs from top n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5000\n",
    "k_pairs = pd.DataFrame(columns=['start','end','wikiDistance','semanticSimilarity'])\n",
    "\n",
    "for i in range(k):\n",
    "\ttemp = df.sample(n=2, random_state=i)\n",
    "\tstart = temp.values[0]\n",
    "\tend = temp.values[1]\n",
    "\n",
    "\tif start not in list(G.nodes) or end not in list(G.nodes):\n",
    "\t\tcontinue\n",
    "\n",
    "\tif nx.has_path(G, start, end):\n",
    "\t\tdist = len(nx.shortest_path(G, start, end)) - 1\n",
    "\telse:\n",
    "\t\tdist = -1\n",
    "\n",
    "\tk_pairs = k_pairs.append({'start':start, 'end':end, 'wikiDistance':dist}, ignore_index=True)\n",
    "\n",
    "k_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [1, 5, 2, 7]\n",
    "similarities = [0.1, 0.5, 0.2, 0.7]\n",
    "\n",
    "print(spearmanr(distances, similarities))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
