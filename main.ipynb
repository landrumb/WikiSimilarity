{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Correlation Between Semantic Similarity and Wikipedia Distance\n",
        "### CMSC 320, Spring 2022\n",
        "#### Ben Landrum, Gaetan Almela, and Nav Bindra\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Wikipedia link graph is a graph where vertices represent Wikipedia articles and directed edges from vertex $A$ to vertex $B$ represent a link to $B$ in the text of $A$. We define Wikipedia distance between pages $A$ and $B$ to be the minimum number of links required to get from $A$ to $B$ in the Wikipedia link graph, where the first such link is on page $A$, the second is on the page the first link points to, and so on, until a link to $B$ is found.\n",
        "\n",
        "Word2Vec is a popular tool for computing word embeddings, which use a vector space representation of words to place them in an abstract space where similar vectors indicate that words appear in similar contexts. These vectors can then be compared to find a metric of semantic similarity between words. \n",
        "\n",
        "Because pages on Wikipedia contain links which are relevant to the topic the page is covering, similar topics are often closely if not directly linked. Therefore, it stands to reason that if semantic similarity is a valid metric of similarity between topics, a higher semantic similarity between two topics should correspond to a lower Wikipedia distance between said topics. We are looking to test the hypothesis that there exists a negative correlation between Wikipedia distance and Word2Vec semantic similarity. Our null hypothesis is that there does not exist a statistically significant negative correlation between Wikipedia distance and Word2Vec semantic similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from requests import get\n",
        "from faker import Faker\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import warnings\n",
        "\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Wikipedia Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Top $n$ Wikipedia Articles\n",
        "\n",
        "English Wikipedia has over 6 million articles. A graph describing the links between all of them would be prohibitively large, so we will only consider the top $n$ articles for some large $n$. For $n > 1,000$ this is not something that can be found online, so we must compute it ourselves.\n",
        "\n",
        "Wikipedia provides data dumps describing page views in the form of files tallying how many views each page received in a given hour. These files are available at [https://dumps.wikimedia.org/other/pageviews/](https://dumps.wikimedia.org/other/pageviews/), and date back to May 1st, 2015. The most popular pages on Wikipedia at any given time are generally determined by current events, so to factor out this bias and get a representative sample of the most popular articles we want to sample the largest possible time period, in this case May 1st, 2015 to the present day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENGLISH_WIKIS = ['en', 'en.m'] # desktop and mobile versions of english wikipedia\n",
        "PAGE_TYPE_REGEX = r'^((User)|(Talk)|(Wikipedia)|(Special)|(Portal)|(File)):' # regex for filtering non-article pages\n",
        "\n",
        "N = 10 ** 4 # number of pages to return (10,000)\n",
        "RANDOM_FILES = 0 # Number of random files to use\n",
        "\n",
        "DATA_DIR = 'data'\n",
        "\n",
        "# pre generated csv of top 1,000 most popular wikipedia pages\n",
        "CACHED_PAGEVIEWS = 'wiki_top.csv.gz'\n",
        "\n",
        "# pre generated graph of the connections between the top 1000 wikipedia pages\n",
        "GRAPH_FILENAME = 'wiki_graph.gml'\n",
        "\n",
        "# our semantic similarity model file, explained later in the project\n",
        "SEM_MODEL = 'crawl-300d-2M.vec'\n",
        "COMPILED_MODEL = 'crawl-300d-2M.d2v'\n",
        "\n",
        "# there constants are only here for the preview. Since this document would take\n",
        "# hours to run, its easier to skip the data collection part and go straight to\n",
        "# the analysis.\n",
        "GET_POP_PAGES = False\n",
        "DOWNLOAD_PAGES = False\n",
        "BUILD_GRAPH = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating a Faker object which will later be used to generate random datetimes\n",
        "fake = Faker()\n",
        "\n",
        "# suppressing a warning caused by use of match groups in a regex \n",
        "warnings.filterwarnings('ignore', message=\"This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\")\n",
        "\n",
        "# generate a random datetime for which there exists a pageview file\n",
        "date = fake.date_time_between(start_date=datetime.datetime(year=2015, month=5, day=1, hour=1), end_date='now')\n",
        "\n",
        "if GET_POP_PAGES:\n",
        "    # get the url of the corresponding pageview file\n",
        "    url = f\"https://dumps.wikimedia.org/other/pageviews/{date.year}/{date.year}-{date.month:02d}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
        "\n",
        "    # download and save the file\n",
        "    seed_file = f\"{DATA_DIR}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
        "    r = get(url)\n",
        "    with open(seed_file, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "    # importing first file to accumulate the others from\n",
        "    all_views = pd.read_csv(seed_file, sep=\" \", header=None, names=[\"project\", \"page\", \"views\", \"null\"], usecols=[\"project\", \"page\", \"views\"])\n",
        "\n",
        "    # filtering to only pages from english wikipedia, both desktop and mobile\n",
        "    en_views = all_views[all_views['project'].isin(ENGLISH_WIKIS)]\n",
        "    # filtering user, talk and other non-article pages\n",
        "    views = en_views[~(en_views['page'].str.contains(PAGE_TYPE_REGEX, na=False))]\n",
        "\n",
        "    # computing views for each page \n",
        "    views = views.groupby('page').sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we get the random files and add their views to the seed file, which becomes increasingly expensive as `all_views` grows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = datetime.datetime.now() # for timing execution\n",
        "\n",
        "if GET_POP_PAGES:\n",
        "    for i in range(RANDOM_FILES - 1):\n",
        "        # generate a random datetime for which there exists a pageview file\n",
        "        date = fake.date_time_between(start_date=datetime.datetime(year=2015, month=5, day=1, hour=1), end_date='now')\n",
        "\n",
        "        # get the url of the corresponding pageview file\n",
        "        url = f\"https://dumps.wikimedia.org/other/pageviews/{date.year}/{date.year}-{date.month:02d}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
        "\n",
        "        # download and save the file\n",
        "        r = get(url)\n",
        "        with open(f\"{DATA_DIR }/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\", 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        print(f\"{date.strftime('%Y/%m/%d:%H')}\")\n",
        "        \n",
        "        # read the file (pandas can handle gzip directly) and perform same operations as above\n",
        "        try:\n",
        "            df = pd.read_csv(f\"{DATA_DIR}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\", compression='gzip', sep=\" \", header=None, names=[\"project\", \"page\", \"views\", \"null\"], usecols=[\"project\", \"page\", \"views\"])\n",
        "        except: # handling error caused by pandas' \"C\" CSV engine\n",
        "            print(f\"{date.strftime('%Y/%m/%d:%H')} failed\" )\n",
        "            os.remove(f\"{DATA_DIR}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\")\n",
        "            continue\n",
        "\n",
        "        df = df[df['project'].isin(ENGLISH_WIKIS)]\n",
        "        df = df[~(df['page'].str.contains(PAGE_TYPE_REGEX, na=False))]\n",
        "        df = df.groupby('page').sum()\n",
        "        # merging with previous dataframe\n",
        "        views = views.add(df, fill_value=0)\n",
        "\n",
        "        # print status report\n",
        "        print(f\"[{i + 1}]\\t{date.strftime('%Y/%m/%d:%H')}\\t{len(views):,} pages\\t[{datetime.datetime.now() - start_time} elapsed, {(i * 100) / RANDOM_FILES:.2f}% complete, {(datetime.timedelta(seconds=((datetime.datetime.now() - start_time).total_seconds() / (i + 1)) * (RANDOM_FILES - i - 1)))} remaining]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we save the pages and their views to a csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if GET_POP_PAGES:\n",
        "  views.sort_values('views', ascending=False, inplace=True)\n",
        "\n",
        "  views.head(N).to_csv(f\"{DATA_DIR}/{CACHED_PAGEVIEWS}\", index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more optimized version of this code was run over 12 hours, sampling a total of 1,105 hours of data. The top 5 million articles from this dataset are in the file `raw_views_5M.csv`, and we'll use them for the rest of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we start scraping Wikipedia, we have to define some constants for our scraper. Scraping Wikipedia is suprisingly easy, the DOM tree and class names are very consistent. Despite this, Wikipedia still has a lot of garbage links that we don't want. This is where we define all of our rules for what counts as a \"bad\" link.  Examples of bad links include images, self links, internal links (links to enlarge photos, believe it or not), or invisible links that take you to the top of the page.\n",
        "\n",
        "One might think to simply filter by links containing `/wiki/`, but even those links aren't safe. Indeed Wikipedia contains various different types of internal `/wiki/` links that point to various medias, template pages, or help files, which we are not interested in. We also filter them here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we filter out these types of link selectors\n",
        "CLEAN = [\n",
        "  'a[id=\"top\"]',\n",
        "  'a[class=\"mw-selflink selflink\"]',\n",
        "  'a[class=\"image\"]',\n",
        "  'a[class=\"internal\"]',\n",
        "]\n",
        "\n",
        "# we filter out these links\n",
        "REM_LINKS = [\n",
        "  r\"(\\/wiki\\/File:\\w+)\",\n",
        "  r\"(\\/wiki\\/Special:\\w+)\",\n",
        "  r\"(\\/wiki\\/Template:\\w+)\",\n",
        "  r\"(\\/wiki\\/Category:\\w+)\",\n",
        "  r\"(\\/wiki\\/Portal:\\w+)\",\n",
        "  r\"(\\/wiki\\/Template_talk:\\w+)\",\n",
        "  r\"(\\/wiki\\/Help:\\w+)\",\n",
        "  r\"(\\/wiki\\/Wikipedia:\\w+)\",\n",
        "  r\"(^#\\w+)\",\n",
        "]\n",
        "\n",
        "# main page content selector\n",
        "CONT_SEL = \"div#content\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we load the list of pages that we are interested in scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load most popular wikipedia pages csv\n",
        "df = pd.read_csv(f\"{DATA_DIR}/{CACHED_PAGEVIEWS}\")\n",
        "\n",
        "# get a list of pages as an array of strings\n",
        "urls = df['page'].to_numpy().astype(str)\n",
        "\n",
        "# filter the pages to only articles without ':' in the title\n",
        "urls = urls[np.char.find(urls, ':') == -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download the Pages Locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To avoid overwhelming Wikipedia we download every page, unmodified, locally. This allows us to experiment with parsing the Wikipedia pages without having to redownload the pages every run. This is also much faster since we are working with local files. The reason we leave the files unmodified here instead of cleaning them at the same time is that we always want to have the maximum amount of information possible first (in case you need it) and clean it up as a separate step.\n",
        "\n",
        "One thing to note here is that we also add a variable `START_INDEX` to manually start from a particular index in our list of links to scrape. This serves two purposes. The first is in case a special page break our code, we don't have to restart from the beginning to rescrape the pages we already have. The second is for this page not to take 50 hours to compile to HTML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# in case you wanna skip ahead\n",
        "START_INDEX = 0\n",
        "SLEEP_TIME_S = 2 # dont make less than 2, we dont wanna overwhelm wikipedia too much\n",
        "\n",
        "n = urls.size\n",
        "\n",
        "# compile the title cleanup regex for optimization\n",
        "title_re = re.compile(r\"\\/\")\n",
        "\n",
        "if DOWNLOAD_PAGES:\n",
        "  for i, url in enumerate(urls):\n",
        "    if i < START_INDEX:\n",
        "      continue\n",
        "\n",
        "    print(f\"{i} of {n} ({round((i / n) * 100, 2)}%) - Scraping {url}\")\n",
        "\n",
        "    # load the page as html with BeautifulSoup\n",
        "    res = get(f'https://en.wikipedia.org/wiki/{url}')\n",
        "\n",
        "    # check if we got baned :c\n",
        "    if res.status_code != 200:\n",
        "      print(\"We got got\")\n",
        "      break\n",
        "\n",
        "    html = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "    # save html page as string\n",
        "    html_str = str(html.prettify())\n",
        "\n",
        "    # replace bad characters in titles with underscores\n",
        "    title = title_re.sub(\"_\", url)\n",
        "\n",
        "    # save file\n",
        "    f = open(f\"{DATA_DIR}/pages/{title}.html\", \"w\")\n",
        "    f.write(html_str)\n",
        "    f.close()\n",
        "\n",
        "    # let's not overload wikipedia with requests here\n",
        "    time.sleep(SLEEP_TIME_S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we use our local data and our filters defined above to build our page connections graph with `networkx`. We iterate through each page in our local file structure and look at each link that matches our filters defined in our constants above. Note that we don't add every url that passes our filter here, _we only add an edge if its destination page is also in our list of pages_. This helps keep the size of our graph reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = os.scandir(path = f\"./{DATA_DIR}/pages/\")\n",
        "n = len(list(files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_path = f\"{DATA_DIR}/{GRAPH_FILENAME}\"\n",
        "\n",
        "G = nx.DiGraph() if BUILD_GRAPH else nx.read_gml(graph_path)\n",
        "\n",
        "if BUILD_GRAPH:\n",
        "  # list our downloaded files\n",
        "  files = os.scandir(path = f\"./{DATA_DIR}/pages/\")\n",
        "\n",
        "  for i, file in enumerate(files):\n",
        "    # extract url from filename\n",
        "    url = Path(file).stem\n",
        "\n",
        "    print(f\"{i} of {n} ({round((i / n) * 100, 2)}%) - Graphing {url}\")\n",
        "\n",
        "    # add the current page to the graph\n",
        "    G.add_node(url)\n",
        "\n",
        "    # load the page into memory\n",
        "    f = open(file, \"r\")\n",
        "    html = BeautifulSoup(f.read(), 'html.parser')\n",
        "    f.close()\n",
        "\n",
        "    # clean up unwanted elements from page\n",
        "    for c in CLEAN:\n",
        "      els = html.select(f\"{CONT_SEL} {c}\")\n",
        "      for el in els:\n",
        "        el.decompose()\n",
        "\n",
        "    # further remove unwanted links\n",
        "    links = html.select(f\"{CONT_SEL} a\")\n",
        "    for link in links:\n",
        "      # extract href from link\n",
        "      href = link['href']\n",
        "\n",
        "      # we are only interested in links that start with `/wiki/` and aren't in REM_LINKS\n",
        "      if not any([re.match(regex, href) for regex in REM_LINKS]) and re.match(r\"^\\/wiki\\/\\w+\", href):\n",
        "        # extract href from link\n",
        "        href = link['href']\n",
        "\n",
        "        # this is the destination url\n",
        "        url_ = re.sub(r\"\\/wiki\\/\", \"\", href)\n",
        "\n",
        "        # we only add the destination url if it happens to be inside our links of interests\n",
        "        if url_ in urls:\n",
        "          # add edge to graph\n",
        "          G.add_edge(url, url_)\n",
        "\n",
        "    # save our graph at other file\n",
        "    if i % 3 == 0:\n",
        "      nx.write_gml(G, graph_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we draw the graph of the top 1000 pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(G.number_of_nodes(), G.number_of_edges())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes = np.array(G.nodes)\n",
        "sub_nodes = np.random.choice(nodes, size = 100)\n",
        "SG = G.subgraph(sub_nodes)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "nodes = (SG.nodes)\n",
        "N_nodes = SG.number_of_nodes()\n",
        "\n",
        "pos = nx.circular_layout(SG)\n",
        "\n",
        "# Setting up the text by using node position \n",
        "texts=[\n",
        "  plt.text(\n",
        "    pos[node][0], \n",
        "    pos[node][1],\n",
        "    str(node),\n",
        "    rotation=(i/N_nodes)*360,\n",
        "    fontsize=10,horizontalalignment='center',\n",
        "    verticalalignment='center'\n",
        "  )\n",
        "  \n",
        "  for i, node in enumerate(nodes)\n",
        "]\n",
        "\n",
        "nx.draw(SG, pos=pos, arrows=True, node_size=50, linewidths=0.6, ax=ax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Semantic Similarity with Word2Vec\n",
        "\n",
        "We now use `gensim` to load a Word2Vec embedding. The embedding used is `crawl-300d-2M.vec`, two million word vectors trained on the 600 billion token Common Crawl corpus. It can be downloaded from https://fasttext.cc/docs/en/english-vectors.html. As previously discussed, the embedding represents words in a 300 dimensional vector space, where the inner product of the vectors of two words corresponds to how commonly they occur in the same documents. \n",
        "\n",
        "Because the Common Crawl is a set of webpages, Wikipedia pages are likely included in the sample. As a result, the presence of a link on a Wikipedia page to another Wikipedia page would cause the titles to be more closely associated in the embedding, because the link and title being in the same page would represent a co-occurrence of the two terms or sets of terms. However, because Wikipedia represents a vanishingly small fraction of the web, the direct influence of this is minimal, and we will disregard it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_BIN = f'{DATA_DIR}/{COMPILED_MODEL}'\n",
        "# save_name = MODEL_BIN[:-4] + \".d2v\"\n",
        "\n",
        "# MODEL = KeyedVectors.load_word2vec_format(MODEL_BIN)\n",
        "# MODEL.save(save_name)\n",
        "\n",
        "# loading model\n",
        "model = KeyedVectors.load(MODEL_BIN)\n",
        "\n",
        "# get words included in model\n",
        "model_keys = model.index_to_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis Test\n",
        "\n",
        "We now proceed to test our hypothesis that the distance between a pair of articles on the Wikipedia graph is correlated with the semantic similarity between the titles of the articles. \n",
        "\n",
        "A slight issue is presented by the fact that Wikipedia distance is directional, whereas the semantic similarity is not. As a result, it's possible that the distance from article $A$ to article $B$ may differ from the distance from article $B$ to article $A$. Because the direction of a pair of articles is sampled randomly, there is no bias introduced by this unidirectional sample, so it's acceptable to compare only one direction's distance to the semantic similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate k article pairs from top n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k = 1000\n",
        "columns = ['start','end','wikiDistance','semanticSimilarity']\n",
        "k_pairs = pd.DataFrame(columns=columns)\n",
        "\n",
        "for i in range(k):\n",
        "\ttemp = df['page'].sample(n=2, random_state=i)\n",
        "\tstart = temp.values[0]\n",
        "\tend = temp.values[1]\n",
        "\n",
        "\tif start not in list(G.nodes) or end not in list(G.nodes):\n",
        "\t\tcontinue\n",
        "\n",
        "\t# compute distance\n",
        "\tif nx.has_path(G, start, end):\n",
        "\t\tdist = len(nx.shortest_path(G, start, end)) - 1\n",
        "\telse:\n",
        "\t\tdist = -1\n",
        "\n",
        "\t# compute semantic similarity\n",
        "\tw1_tokens = start.split('_') # split into tokens\n",
        "\tw1_tokens = list(filter(lambda x: x in model_keys, w1_tokens)) # exclude terms not in embedding\n",
        "\n",
        "\tw2_tokens = end.split('_') \n",
        "\tw2_tokens = list(filter(lambda x: x in model_keys, w2_tokens))\n",
        "\n",
        "\t# if the resulting page name is empty (none of the words are in our sem model), then ignore that page\n",
        "\tif len(w1_tokens) == 0 or len(w2_tokens) == 0:\n",
        "\t\tcontinue\n",
        "\t\n",
        "\tsemSim = model.n_similarity(w1_tokens, w2_tokens)\n",
        "\n",
        "\tnew_row = pd.DataFrame({'start': [start], 'end': [end], 'wikiDistance': [dist], 'semanticSimilarity':[semSim]})\n",
        "\n",
        "\tk_pairs = pd.concat([k_pairs, new_row])\n",
        "\t# k_pairs = pd.concat(k_pairs, {'start': start, 'end': end, 'wikiDistance': dist, 'semanticSimilarity':semSim}, ignore_index=True)\n",
        "\n",
        "k_pairs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use Spearman's r test to compute the probability that there exists a statistically significant correlation between WikiDistance and Semantic Similarity. We should note that the Spearman correlation coefficent differs from the more standard Pearson correlation coefficent in that it measures how well a relationship is described by some monotonic function, as opposed to the Pearson coefficient which specifically measures the strength and direction of a linear relationship. We opt to use this measure of correlation because we have no reason to assume that a relationship between WikiDistance and Semantic Similarity will be linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr, p_val = spearmanr(k_pairs['wikiDistance'], k_pairs['semanticSimilarity'])\n",
        "print(f\"correlation: {corr} p-value: {p_val}  \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reasoning about our answer, we can conclude that WikiDistance and Semantic Similarity are weakly, negatively correlated. That being said, our p-value is incredibly small due to our large sample size. Thus, we can can confidently conclude that there does exist some relationship between WikiDistance and Semantic Similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This correlation can be visualized on a violin plot of the semantic similarities of article pairs at each observed distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distances = [i for i in range(1, 8)]\n",
        "similarities = [k_pairs[k_pairs['wikiDistance'] == i]['semanticSimilarity'].array for i in distances]\n",
        "\n",
        "plt.violinplot(similarities, positions=distances, showmeans=True, showextrema=True)\n",
        "plt.xticks(distances, distances)\n",
        "plt.xlabel('Wiki Distance')\n",
        "plt.ylabel('Semantic Similarity')\n",
        "plt.title('Semantic Similarity vs. Wiki Distance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
