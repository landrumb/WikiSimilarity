{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Between Semantic Similarity and Wikipedia Distance\n",
    "### CMSC 320, Spring 2022\n",
    "#### Ben Landrum, Gaetan Almela, and Nav Bindra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikipedia link graph is a graph where vertices represent Wikipedia articles and directed edges from vertex $A$ to vertex $B$ represent a link to $B$ in the text of $A$. We define Wikipedia distance between pages $A$ and $B$ to be the minimum number of links required to get from $A$ to $B$ in the Wikipedia link graph, where the first such link is on page $A$, the second is on the page the first link points to, and so on, until a link to $B$ is found.\n",
    "\n",
    "Word2Vec is a popular tool for computing word embeddings, which use a vector space representation of words to place them in an abstract space where similar vectors indicate that words appear in similar contexts. These vectors can then be compared to find a metric of semantic similarity between words. \n",
    "\n",
    "Because pages on Wikipedia contain links which are relevant to the topic the page is covering, similar topics are often closely if not directly linked. Therefore, it stands to reason that if semantic similarity is a valid metric of similarity between topics, a higher semantic similarity between two topics should correspond to a lower Wikipedia distance between said topics. We are looking to test the hypothesis that there exists a negative correlation between Wikipedia distance and Word2Vec semantic similarity. Our null hypothesis is that there does not exist a statistically significant negative correlation between Wikipedia distance and Word2Vec semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Semantic Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This is not hard. If we wanted to pad it we could compute our own embedding, which would probably make it seem a little less trivial and would honestly be really easy, just involves a lot of text being loaded.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Wikipedia Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top $n$ Wikipedia Articles\n",
    "\n",
    "English Wikipedia has over 6 million articles. A graph describing the links between all of them would be prohibitively large, so we will only consider the top $n$ articles for some large $n$. For $n > 1,000$ this is not something that can be found online, so we must compute it ourselves.\n",
    "\n",
    "Wikipedia provides data dumps describing page views in the form of files tallying how many views each page received in a given hour. These files are available at [https://dumps.wikimedia.org/other/pageviews/](https://dumps.wikimedia.org/other/pageviews/), and date back to May 1st, 2015. The most popular pages on Wikipedia at any given time are generally determined by current events, so to factor out this bias and get a representative sample of the most popular articles we want to sample the largest possible time period, in this case May 1st, 2015 to the present day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from faker import Faker\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "ENGLISH_WIKIS = ['en', 'en.m'] # desktop and mobile versions of english wikipedia\n",
    "PAGE_TYPE_REGEX = '^((User)|(Talk)|(Wikipedia)|(Special)|(Portal)|(File)):' # regex for filtering non-article pages\n",
    "N = 10 ** 4 # number of pages to return\n",
    "OUTPUT_FILE = 'ten_k_most_viewed_pages.csv' # name of output file\n",
    "DATA_DIR = 'Data' # directory where the data is stored\n",
    "RANDOM_FILES = 6 # Number of random files to use\n",
    "\n",
    "# creating a Faker object which will later be used to generate random datetimes\n",
    "fake = Faker()\n",
    "\n",
    "# suppressing a warning caused by use of match groups in a regex \n",
    "warnings.filterwarnings('ignore', message=\"This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random datetime for which there exists a pageview file\n",
    "date = fake.date_time_between(start_date=datetime.datetime(year=2015, month=5, day=1, hour=1), end_date='now')\n",
    "\n",
    "# get the url of the corresponding pageview file\n",
    "url = f\"https://dumps.wikimedia.org/other/pageviews/{date.year}/{date.year}-{date.month:02d}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
    "\n",
    "# download and save the file\n",
    "seed_file = DATA_DIR + \"/\" + f\"pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
    "r = requests.get(url)\n",
    "with open(seed_file, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# importing first file to accumulate the others from\n",
    "all_views = pd.read_csv(seed_file, sep=\" \", header=None, names=[\"project\", \"page\", \"views\", \"null\"], usecols=[\"project\", \"page\", \"views\"])\n",
    "\n",
    "# filtering to only pages from english wikipedia, both desktop and mobile\n",
    "en_views = all_views[all_views['project'].isin(ENGLISH_WIKIS)]\n",
    "# filtering user, talk and other non-article pages\n",
    "views = en_views[~(en_views['page'].str.contains(PAGE_TYPE_REGEX, na=False))]\n",
    "\n",
    "# computing views for each page \n",
    "views = views.groupby('page').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the random files and add their views to the seed file, which becomes increasingly expensive as `all_views` grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017/04/07:13\n",
      "[1]\t2017/04/07:13\t2,448,411 pages\t[0:00:22.516213 elapsed, 0.00% complete, 0:00:22.516213 remaining]\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now() # for timing execution\n",
    "for i in range(RANDOM_FILES - 1):\n",
    "    # generate a random datetime for which there exists a pageview file\n",
    "    date = fake.date_time_between(start_date=datetime.datetime(year=2015, month=5, day=1, hour=1), end_date='now')\n",
    "\n",
    "    # get the url of the corresponding pageview file\n",
    "    url = f\"https://dumps.wikimedia.org/other/pageviews/{date.year}/{date.year}-{date.month:02d}/pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\"\n",
    "\n",
    "    # download and save the file\n",
    "    r = requests.get(url)\n",
    "    with open(DATA_DIR + \"/\" + f\"pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\", 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"{date.strftime('%Y/%m/%d:%H')}\")\n",
    "    \n",
    "    # read the file (pandas can handle gzip directly) and perform same operations as above\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_DIR + \"/\" + f\"pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\", compression='gzip', sep=\" \", header=None, names=[\"project\", \"page\", \"views\", \"null\"], usecols=[\"project\", \"page\", \"views\"])\n",
    "    except: # handling error caused by pandas' \"C\" CSV engine\n",
    "        print(f\"{date.strftime('%Y/%m/%d:%H')} failed\" )\n",
    "        os.remove(DATA_DIR + \"/\" + f\"pageviews-{date.strftime('%Y%m%d')}-{date.hour:02d}0000.gz\")\n",
    "        continue\n",
    "\n",
    "    df = df[df['project'].isin(ENGLISH_WIKIS)]\n",
    "    df = df[~(df['page'].str.contains(PAGE_TYPE_REGEX, na=False))]\n",
    "    df = df.groupby('page').sum()\n",
    "    # merging with previous dataframe\n",
    "    views = views.add(df, fill_value=0)\n",
    "\n",
    "    # print status report\n",
    "    print(f\"[{i + 1}]\\t{date.strftime('%Y/%m/%d:%H')}\\t{len(views):,} pages\\t[{datetime.datetime.now() - start_time} elapsed, {(i * 100) / RANDOM_FILES:.2f}% complete, {(datetime.timedelta(seconds=((datetime.datetime.now() - start_time).total_seconds() / (i + 1)) * (RANDOM_FILES - i - 1)))} remaining]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the pages and their views to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views.sort_values('views', ascending=False, inplace=True)\n",
    "\n",
    "views.head(N).to_csv(OUTPUT_FILE, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the form shown here, this code only samples data from 6 hours, but for a more complete sample a more optimized version of this code was run over 12 hours, sampling a total of 1,105 hours of data. The top 5 million articles from this dataset are in the file `raw_views_5M.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This will be very computationally expensive for articles which are far apart or have particularly high degrees, and will require the loading of a prohibitively large database of links between Wikipedia articles. Said database should probably be on a local SQL server.__\n",
    "\n",
    "An SQL database of links between Wikipedia articles can be downloaded from [https://dumps.wikimedia.org/enwiki/latest/](Wikipedia's data dumps). The copy of `enwiki-latest-pagelinks.sql` used here was generated on April 20th, 2022, and is reasonably current to the state of Wikipedia at that time. The database is a table of links between Wikipedia articles, with each row representing a link from one article to another.\n",
    "<!-- \n",
    "In order to generate the SQLite database we're using to access the graph, we run the following on the command line:\n",
    "```\n",
    "sqlite3 pagelinks.db \".read enwiki-latest-pagelinks.sql\"\n",
    "```\n",
    "This takes forever, but allows us to access the graph with SQLite. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bot With Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This is virtually already done, but we can make it much better and I would love to compare different iterations a la chess bot comparison grid.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Semantic Similarity to Approximate Wikipedia Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This seems pointless but makes the statistical analysis straightforward.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can just compute the two values for random pairs of articles until we have a super low p-value and say boom statistically significant correlation. Will have to consult stat notes to remember what test you're supposed to use for correlation. $\\chi^2$?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
