{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some constants..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "\n",
    "# we filter out these elements\n",
    "CLEAN = [\n",
    "  'a[id=\"top\"]',\n",
    "  'a[class=\"mw-selflink selflink\"]',\n",
    "  'a[class=\"image\"]',\n",
    "  'a[class=\"internal\"]',\n",
    "  \"sup\",\n",
    "  \"div.reflist\" # remove citations, that doesn't count\n",
    "]\n",
    "\n",
    "# we filter out these links\n",
    "REM_LINKS = [\n",
    "  r\"(\\/wiki\\/File:\\w+)\",\n",
    "  r\"(\\/wiki\\/Special:\\w+)\",\n",
    "  r\"(\\/wiki\\/Template:\\w+)\",\n",
    "  r\"(\\/wiki\\/Category:\\w+)\",\n",
    "  r\"(\\/wiki\\/Portal:\\w+)\",\n",
    "  r\"(\\/wiki\\/Template_talk:\\w+)\",\n",
    "  r\"(\\/wiki\\/Help:\\w+)\",\n",
    "  r\"(\\/wiki\\/Wikipedia:\\w+)\",\n",
    "  r\"(^#\\w+)\",\n",
    "]\n",
    "\n",
    "# main page content selector\n",
    "CONT_SEL = \"div#content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a function to clean up the page of any unwanted links or elements. Although Wikipedia pages are fairly clean and nice to work with programmatically, there are still certain types of elements that we want to filter out. Such links include self links (links that link back to themselves), image links, internal links, link to files or template pages, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_page(html):\n",
    "  # clean up unwanted links from pages\n",
    "  for c in CLEAN:\n",
    "    els = html.select(f\"{CONT_SEL} {c}\")\n",
    "    for el in els:\n",
    "      el.decompose()\n",
    "\n",
    "  # format remaining links\n",
    "  links = html.select(f\"{CONT_SEL} a\")\n",
    "  for link in links:\n",
    "    # extract href from link\n",
    "    href = link['href']\n",
    "\n",
    "    # extract text of links and remove punctuation\n",
    "    text = re.sub(r\"[\\,\\.\\:\\!\\?]\", \"\", link.text)\n",
    "\n",
    "    # at this stage, we want to further remove certain types of links\n",
    "    # that is: any of the links in REM_LINKS, OR any link that doesn't start with /wiki/\n",
    "    if any([re.match(regex, href) for regex in REM_LINKS]) or not re.match(r\"^\\/wiki\\/\\w+\", href):\n",
    "      link.decompose()\n",
    "    else:\n",
    "      # remove leading /wiki/ from href as it is redundant\n",
    "      href = re.sub(r\"\\/wiki\\/\", \"\", href)\n",
    "\n",
    "      # Here is the 1000 IQ play. We want to preserve the URL of the links but\n",
    "      # also work with them from a cleaner text file. We CAN extract the text\n",
    "      # from the entire page but that would mean losing the hrefs. To solve\n",
    "      # this, we replace the text content of the link with its text AND the\n",
    "      # associated href. THEN we can simply extract the text content of the file\n",
    "      # without losing the href!!1\n",
    "      link.replace_with(f'{{{text}|{href}}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load the list of pages that we are interested in scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load most popular wikipedia pages csv\n",
    "df = pd.read_csv(f\"{DATA_DIR}/top1000.csv\")\n",
    "\n",
    "# get a list of pages as an array of strings\n",
    "pages = df['article'].to_numpy().astype(str)\n",
    "\n",
    "# filter the pages to only articles without ':' in the title\n",
    "# I know this may not cover everything, but I'm just testing here\n",
    "pages = pages[np.char.find(pages, ':') == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you wanna skip ahead\n",
    "START_INDEX = 0\n",
    "SLEEP_TIME_S = 2\n",
    "\n",
    "n = pages.size\n",
    "\n",
    "# compile this for efficiency\n",
    "newline_regex = re.compile(r\"\\n{3,}\")\n",
    "\n",
    "for i, page in enumerate(pages):\n",
    "  if i < START_INDEX:\n",
    "    continue\n",
    "\n",
    "  print(f\"{i} of {n} ({round((i / n) * 100, 2)}%) - Now Scraping {page}\")\n",
    "\n",
    "  # load the page as html with BeautifulSoup\n",
    "  res = get(f'https://en.wikipedia.org/wiki/{page}')\n",
    "\n",
    "  # check if we got baned :c\n",
    "  if res.status_code != 200:\n",
    "    print(\"We got got\")\n",
    "    break\n",
    "\n",
    "  html = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "  # clean up html on the page\n",
    "  cleanup_page(html)\n",
    "\n",
    "  # create one parsed page and one clean html page\n",
    "  parsed_page = newline_regex.sub(\"\\n\\n\", html.getText()) # replace any more than three newlines into only 2\n",
    "  html_page = str(html.prettify())\n",
    "\n",
    "  # replace bad characters in titles with underscores\n",
    "  title = re.sub(r\"\\/\", \"_\", page)\n",
    "\n",
    "  # save files\n",
    "  parsed_file = open(f\"{DATA_DIR}/pages/text/{i + 1}-{title}.txt\", \"w\")\n",
    "  parsed_file.write(parsed_page)\n",
    "  parsed_file.close()\n",
    "\n",
    "  html_file = open(f\"{DATA_DIR}/pages/html/{i + 1}-{title}.html\", \"w\")\n",
    "  html_file.write(html_page)\n",
    "  html_file.close()\n",
    "\n",
    "  # let's not overload wikipedia with requests here\n",
    "  time.sleep(SLEEP_TIME_S)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
